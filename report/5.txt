5) Now present some experimental results from applying your discretized
PDE methods on data. The data can be synthetically generated or real data
(real data is not important for this project). If you are exploring more
than one PDE model (and/or testing the effect of changing some parameters in
your PDE model), then make sure you present multiple experimental results.
Attempt to extract/present some quantitative information to compare results
rather than just showing only images. For example, if you are doing anisotropic
image diffusing to preserve edges, you can devise any sort of reasonable
quantitative measure to compare the edge preservation effects between
different PDE choices, and then present a table of the comparisons (or a plot
showing the way this value changes with the number of PDE iterations for
each choice of your PDE).
--------------------------------------------------------------------------------

\section{Experiments}

\subsection{Image used in this study}

The image studied in this report pictures a hedgehog, and is taken from an online source(https://www.pinterest.com/pin/315814992604976841/).

It was chosen because of the variety of its features : its pikes present many edges, which provide regions of high gradients. There are also smooth regions, such as the eyes and the nose of the hedgehog. Some areas are relatively smooth but present a very fine texture, such as the watermelon. Adding noise to this area will make it difficult to reconstruct via super-resolution. 

We study a small Region of Interest (ROI) of the initial image, to help with the computation speed and the analysis. This ROI is centered on the face of the hedgehog: it presents small pikes and smooth areas. This region is also in the focus of the camera, and presents more details than the rest of the image, which is slightly out of focus.

To measure the deformation of the reconstructed image compared to the original one, three metrics will be used.
The Mean Squared Error (MSE) measures the absolute distance between the two images, and represents the absolute error made during the reconstruction process. For a good quality, it should be as low as possible. 
The Peak Signal-to-Noise Ratio(PSNR) is linked to the MSE. It evaluates the influence of noise in the image, and is expressed in decibels (dB). The higher the PSNR, the lower the noise disturbs the image.
A more sophisticated measure is the Structural Similarity Index. It compares the difference of perception between two images, by measuring how the structural difference between images is perceived. This index has values between $0$ and $1$, and exactly similar images have a SSIM of $1$.


\subsection{Data fidelity versus regularization}

In this section, we will study the quality of the image reconstucted by super resolution while varying the coefficient $\lambda$. Modifying this parameter allows us to know the relative importance of the regularization term compared to the data fidelity term in the gradient descent equation to get a good image. We can then find the optimal value of $\lambda$ for our model.

The values of $\lambda$ tested are $1, 1.5, 2, 3, 4, 5, 6, 7, 10$. The other parameters of the experiment were _______.
We measured for each experiment the MSE, PSNR and SSIM of the resulting SR recstruction compared to the original one. The following figures on the left show the results, and the figures on the right show a zoom of the end of the graph on its left.
%figures in 3*2 table

These figures show that all the experiments converged to a minimum MSE after 750 iterations but $\lambda = 1$. This particular value shows worse results: its PSNR and SSIM are worse than the other values of $\lambda$. This particular value, which weights the data fidelity term and the regularization term evenly, seems to show the conflicting influence of the two terms. Apart from this term, the convergence speed of all $\lambda$ values is similar.

We notice on the zoom figures that the best $\lambda$ value is $\lambda = 6$, with some good error metrics: $MSE = $, $PSNR = 26.8dB$, $SSIM = 0.56$. The similarity index is not very high, even if our image looks correct, because of the loss of details during reconstruction. For all future experiments, we fix the parameter $\lambda = 6$.

The pictures below are the original, and the reconstructions after 750 iterations with $\lambda = 1$ and $\lambda = 6$. A higher noise level is visible in the picture with $\lambda = 1$.
3 figures

\subsection{Comparing adaptative weighting schemes}

Here, we test the reconstructed images quality for both weighting functions $\tau_1$ and $\tau_2$ for different values of $C$, and we compare the results to find the best configuration for our experiment.

Similarly, we run 750 iterations of gradient descent with $\lambda = 6$, ____common parameters______.
We measured the distorsion of the reconstruction using the same metrics as before, and the layout of our presentation is similar.
% figures in 3*2 table.
Both weighting schemes show similar evolution. In particular, the convergence speed of all options is similar, and all solutions reach similar results. As we can see on the graphs on the right, the final convergence is not fully reached yet. 

Comparing all final values, we find that the configuration $C = \frac{1}{6}$ produce both the lowest MSE and the highest SSIM in both schemes. The results are slightly better  with $\tau_1$ than with $\tau_2$.

Below are the pictures of the original ROI, the result of 200 iterations using $\tau_1(C=\frac{1}{6}$, and the result after 750 iterations using $\tau_1(C=\frac{1}{6}$. 
%3 pictures


